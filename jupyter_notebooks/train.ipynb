{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['dark_background'])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.layers import Input, Conv2D, Conv1D, AveragePooling1D, AveragePooling2D, Dense, Flatten, Dropout, BatchNormalization, MaxPool1D, Activation, concatenate\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "debug_mode = 1\n",
    "\n",
    "from clr import CyclicLR\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc \n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "NAME_OF_NOTEBOOK = 21\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    # This returns a tensor\n",
    "    inputs = Input(shape=(55,300))\n",
    "    input_head = Input(shape=(6,))\n",
    "\n",
    "    x = Dense(256)(inputs)\n",
    "    x = Activation('elu')(x)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = Activation('elu')(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "    x2 = Dense(16)(input_head) \n",
    "    #x2 = BatchNormalization()(x2)\n",
    "    x2 = Activation('elu')(x2)\n",
    "\n",
    "    x = concatenate([x,x2])\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    #x = Dense(128, activation='relu')(x)\n",
    "    predictions = Dense(55, activation='linear')(x)\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and three Dense layers\n",
    "    model = Model(inputs=[inputs,input_head], outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer='nadam',\n",
    "                  loss='mse',\n",
    "                  )\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X_train_head = np.load('all_train_heads.npy')\n",
    "    X_train = np.load('all_train.npy')\n",
    "    Y = np.load('all_targets.npy')\n",
    "    return X_train_head, X_train, Y\n",
    "\n",
    "# def load_test_data():\n",
    "#     X_test_head = np.load('all_test_heads.npy')\n",
    "#     scaler_filename = \"scaler.save\"\n",
    "#     sk = joblib.load(scaler_filename)\n",
    "#     X_test_head = sk.transform(X_test_head)\n",
    "\n",
    "#     X_test = np.load('all_test.npy')\n",
    "#     X_test = X_test.reshape(-1,55*300)\n",
    "#     X_test = (X_test-1)*100\n",
    "#     X_test = X_test.reshape(-1,55,300)\n",
    "#     return X_test_head, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_head, X_train, Y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.save']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1,55*300)\n",
    "X_train = (X_train-1)*1000\n",
    "X_train = X_train.reshape(-1,55,300)\n",
    "\n",
    "sk = StandardScaler()\n",
    "scaler_filename = \"scaler.save\"\n",
    "X_train_head = sk.fit_transform(X_train_head)\n",
    "joblib.dump(sk, scaler_filename) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_head, X_test = load_test_data()\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_keras(X_train, X_train_head, Y, train=True):\n",
    "    \n",
    "    global NAME_OF_NOTEBOOK\n",
    "    \n",
    "    kf = KFold(n_splits=10, random_state=23, shuffle=True)\n",
    "    mse_scores = []\n",
    "    for j, (train_idx, val_idx) in enumerate(kf.split(X_train_head)):\n",
    "        #print(\"TRAIN:\", train_idx, \"TEST:\", val_idx)\n",
    "        #x_train, x_train_head, x_test, x_test_head = X_train[train_idx], X_train_head[train_idx], X_train[val_idx], X_train_head[val_idx]\n",
    "        #y_train, y_test = Y[train_idx]*1000, Y[val_idx]*1000\n",
    "\n",
    "        # get a model\n",
    "        mdl = model()\n",
    "\n",
    "        debug_mode = 1\n",
    "        \n",
    "        whts_file = \"./weights/{}_CV_weights_fold_{}.h5\".format(NAME_OF_NOTEBOOK, j)\n",
    "\n",
    "        if train:\n",
    "            # calbacks\n",
    "            logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 23, verbose = debug_mode, mode= 'min')\n",
    "            checkpoint = keras.callbacks.ModelCheckpoint(whts_file, monitor='val_loss', verbose = debug_mode, save_best_only=True, mode='min')\n",
    "\n",
    "            clr = CyclicLR(base_lr=0.0009, max_lr=0.03,\n",
    "                                    step_size=301., mode='triangular2')\n",
    "            calls = [tensorboard_callback, early_stopping, checkpoint, clr, keras.callbacks.TerminateOnNaN()]\n",
    "        \n",
    "            mdl.fit([ X_train[train_idx], X_train_head[train_idx]], Y[train_idx]*1000, epochs=1000, batch_size=3048,\n",
    "                  validation_data=([ X_train[val_idx], X_train_head[val_idx]], Y[val_idx]*1000), callbacks=calls)\n",
    "\n",
    "        # load best weights \n",
    "        mdl.load_weights(whts_file)\n",
    "\n",
    "        # prediction on validation dataset\n",
    "        y_pred = mdl.predict([X_train[val_idx], X_train_head[val_idx]])\n",
    "        score = mean_squared_error(Y[val_idx]*1000, y_pred)\n",
    "        mse_scores.append(score)\n",
    "        print(\"FOLD-{}, MSE: {} \\n\".format(j+1, score))\n",
    "\n",
    "        # save some place \n",
    "        del y_pred, mdl\n",
    "        gc.collect()\n",
    "    print(\"MSE \", mse_scores)\n",
    "    print('\\n AVERAGE MSE: ', np.mean(mse_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD-1, MSE: 10.209456952296684 \n",
      "\n",
      "FOLD-2, MSE: 10.718903233534231 \n",
      "\n",
      "FOLD-3, MSE: 9.838143423048567 \n",
      "\n",
      "FOLD-4, MSE: 9.216341205803587 \n",
      "\n",
      "FOLD-5, MSE: 9.788903213255434 \n",
      "\n",
      "FOLD-6, MSE: 8.521990591126913 \n",
      "\n",
      "FOLD-7, MSE: 10.567978675195748 \n",
      "\n",
      "FOLD-8, MSE: 8.688548324320623 \n",
      "\n",
      "FOLD-9, MSE: 8.738196119911331 \n",
      "\n",
      "FOLD-10, MSE: 10.056875032708389 \n",
      "\n",
      "MSE  [10.209456952296684, 10.718903233534231, 9.838143423048567, 9.216341205803587, 9.788903213255434, 8.521990591126913, 10.567978675195748, 8.688548324320623, 8.738196119911331, 10.056875032708389]\n",
      "\n",
      " AVERAGE MSE:  9.634533677120151\n"
     ]
    }
   ],
   "source": [
    "CV_keras(X_train, X_train_head, Y, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
